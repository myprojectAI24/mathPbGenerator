services:
  vllm:
    image: vllm/vllm-openai:v0.13.0
    ports:
      - "8085:8000"
    volumes:
      - ./models:/app/model
    environment:
      - OMP_NUM_THREADS=8
      - VLLM_WORKER_MULTIPROC_METHOD=spawn
      - VLLM_LOGGING_LEVEL=INFO
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
    command:
      - /app/model/hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4
      # - --dtype
      # - half
      - --tokenizer-mode
      - auto
      - --trust-remote-code
      # - --quantization
      # - awq
      - --tensor-parallel-size
      - "1"
      - --gpu-memory-utilization
      - "0.75"
      - --max-model-len
      - "8192"
      - --enforce-eager
      # - --disable-custom-all-reduce
    ipc: host
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
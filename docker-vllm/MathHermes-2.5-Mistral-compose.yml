services:
  vllm:
    image: vllm/vllm-openai:v0.13.0
    ports:
      - "8085:8000"
    volumes:
      - ./models:/app/model
    environment:
      - OMP_NUM_THREADS=8
      - VLLM_WORKER_MULTIPROC_METHOD=spawn
      - VLLM_LOGGING_LEVEL=INFO
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
    command:
      - /app/model/RichardErkhov/simonveitner_-_MathHermes-2.5-Mistral-7B-awq
      - --dtype
      - half
      - --tokenizer-mode
      - auto
      - --trust-remote-code
      - --quantization
      - awq_marlin
      - --tensor-parallel-size
      - "2"
      - --gpu-memory-utilization
      - "0.85"
      - --max-model-len
      - "12000"
      - --enforce-eager
      # - --disable-custom-all-reduce

    ipc: host
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped